# Load testing with Locust
This repository includes a simple Locust setup to stress test the FastAPI inference server.

## Index
- [Folder layout](#folder-layout)
- [Prerequisites](#prerequisites)
- [Prepare test data](#prepare-your-test-data)
- [Configuration (config.json)](#configuration-configjson)
- [Run Locust (headless)](#run-locust-headless)
- [Notes](#notes)
- [Example config.json](#example-configjson)

## Folder layout
- test-server/
  - locustfile.py – Locust scenarios that read test-server/test_files/splits/<dataset>/X_test.parquet and send requests to /invocation
  - test_files/ – contains splits/<dataset>/ folders with Parquet files produced by training-scripts (X_train.parquet, X_test.parquet, y_train.parquet, y_test.parquet)
  - run_locust_headless.sh – convenience script to run Locust in headless mode

## Prerequisites
- Server running (see earlier sections) on http://localhost:8000 or another host
- Dependencies installed (from repository root):
  - pip install -r requirements.txt

## Prepare your test data
- Ensure the folder test-server/test_files/splits/<dataset>/ exists with Parquet files generated by training. The easiest way is to run from the repository root:
  ```bash
  bash start.sh train
  ```
  This trains the example datasets and saves splits under experiment-results/splits/<dataset>/.
- Required file for load testing: test-server/test_files/splits/<dataset>/X_test.parquet
- If your splits are under experiment-results/splits/<dataset>/, copy them into test-server/test_files/splits/<dataset>/:
  ```bash
  # from repository root
  mkdir -p test-server/test_files/splits/<dataset>
  cp experiment-results/splits/<dataset>/X_test.parquet test-server/test_files/splits/<dataset>/
  # optional (not required by Locust):
  cp experiment-results/splits/<dataset>/y_test.parquet test-server/test_files/splits/<dataset>/
  ```

## Configuration (config.json)
- Locust defaults can be configured in test-server/config.json
- Fields:
  - host: base URL of the model server (used if --host is not provided)
  - users: default total users (documentation only; use CLI flags to apply)
  - spawn_rate: default user spawn rate per second (documentation only)
  - duration: default test duration (documentation only)
  - stop_timeout: default graceful stop timeout in seconds (documentation only)
  - wait_time.min_seconds / max_seconds: controls per-user wait times between tasks
  - pred_method: optional prediction method to append as /invocation?method=...
  - dataset_name: which dataset splits to use under test_files/splits/
- Precedence:
  - CLI flags/environment always override config.json.
  - PRED_METHOD env var overrides pred_method in config.
  - DATASET_NAME env var overrides dataset_name in config.

## Run Locust (headless)
Option A – use the helper script (defaults to testing ALL datasets and models):
```bash
# from repository root
bash start.sh test http://localhost:8000 200 20 2m DEBUG
```
- You will be prompted for dataset and model selections; both default to `all`. The script iterates over all 4 datasets × 3 models and repeats per your chosen run count.
- Arguments: HOST USERS SPAWN_RATE DURATION [LOGLEVEL]
  - HOST: default http://localhost:8000
  - USERS: total concurrent users (e.g., 200)
  - SPAWN_RATE: users spawned per second (e.g., 20)
  - DURATION: test time (e.g., 2m, 5m, 1h)
  - LOGLEVEL: Locust logging level (default INFO). Examples: INFO, DEBUG, WARNING
- Optional: export PRED_METHOD=predict_proba to have Locust call /invocation?method=predict_proba
- Alternatively, set the LOGLEVEL environment variable instead of passing it as the 5th argument.

Option B – raw helper script without the start.sh wrapper:
```bash
bash test-server/run_locust_headless.sh http://localhost:8000 200 20 2m DEBUG
```

Option B – raw Locust command:
```bash
locust -f test-server/locustfile.py \
  --headless \
  --host http://localhost:8000 \
  -u 200 \
  -r 20 \
  -t 2m \
  --stop-timeout 60
```

## Notes
- locustfile.py reads Parquet features from test-server/test_files/splits/<dataset>/X_test.parquet.
- Select the dataset by setting DATASET_NAME environment variable or via start.sh prompt.
- The script sends only single-row requests to POST /invocation. No /health checks are performed.
- For a more aggressive test, increase USERS and SPAWN_RATE and reduce wait times if needed.

## Example config.json
```json
{
  "host": "http://localhost:8000",
  "users": 200,
  "spawn_rate": 20,
  "duration": "2m",
  "stop_timeout": 60,
  "wait_time": { "min_seconds": 0.01, "max_seconds": 0.1 },
  "pred_method": null,
  "dataset_name": "credit_card_transactions"
}
```

